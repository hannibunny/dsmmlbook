
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Ensemble Methods &#8212; Machine Learning (DSM)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hyperparameter Optimisation with Scikit-Learn" href="05Optimisation.html" />
    <link rel="prev" title="Learn evaluate and compare Classification Models" href="03ClassificationPipe.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning (DSM)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Dieses Buch durchsuchen ..." aria-label="Dieses Buch durchsuchen ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Overview.html">
   Machine Learning with Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning with Scikit-Learn
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02RegressionPipe.html">
   Learn and evaluate a Regression Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03ClassificationPipe.html">
   Learn evaluate and compare Classification Models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Ensemble Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05Optimisation.html">
   Hyperparameter Optimisation with Scikit-Learn
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01NeuralNetworkImplementation.html">
   Multi Layer Perceptron for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06KerasLSTMbikeRentalPrediction.html">
   LSTM Time Series Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="12FeatureSelection.html">
   Feature Selection and Extraction
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navigation umschalten" aria-controls="site-navigation"
                title="Navigation umschalten" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Laden Sie diese Seite herunter"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Lecture/05EnsembleMethods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Quelldatei herunterladen" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="In PDF drucken"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Vollbildmodus"
        title="Vollbildmodus"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Lecture/05EnsembleMethods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Starten Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Inhalt
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bagging">
   Bagging
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest">
     Random Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extremely-randomized-trees">
     Extremely Randomized Trees
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#boosting">
   Boosting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adaboost">
     Adaboost
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-tree-boosting">
     Gradient Tree Boosting
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Ensemble Methods</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Inhalt </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bagging">
   Bagging
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-forest">
     Random Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extremely-randomized-trees">
     Extremely Randomized Trees
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#boosting">
   Boosting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adaboost">
     Adaboost
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-tree-boosting">
     Gradient Tree Boosting
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="ensemble-methods">
<h1>Ensemble Methods<a class="headerlink" href="#ensemble-methods" title="Permalink to this headline">¶</a></h1>
<p>In machine learning ensemble methods combine multiple learners into a single one. The underlying idea is that <strong>a complex task can be better mastered by an ensemble as by an individual</strong>, if each member of
the ensemble learns something different. A group of experts is supposed to be better than an individual with a quite broad knowledge. Obviously, a group of experts is useless, if all experts know the same. <strong>The
individual knowledge of the ensemble members should be as diverse as possible.</strong> This requirement can be fulllled by the following approaches:</p>
<ul class="simple">
<li><p>Apply <strong>different training algorithms</strong> (e.g. linear classiers, neural networks, SVMs, …) and/or different configurations of the training algorithms</p></li>
<li><p>Use <strong>different training sets</strong> for training the individual models and/or weight the samples of the training sets for each learner in a different way. However, for all variants of this type the set of used features is the same</p></li>
<li><p>For the individual learners use different representations, i.e. <strong>different feature subsets</strong></p></li>
</ul>
<p>The single learners which constitute the ensemble are usually quite simple. E.g. a common approach is to apply decision trees as weak learners.</p>
<p>The different types of ensemble learning methods differ in the way how the individual training-sets are designed and how they combine their learned knowledge.
On an abstract level two different categories of ensemble learning are distinguished:</p>
<ul class="simple">
<li><p>In the <strong>parallel</strong> approach each member performs individual training from scratch. The individual models are combined by weighted averaging. Methods of this class usually apply <strong>bagging</strong> for trainingdata selection.</p></li>
<li><p>Boosting models are build by <strong>sequentially</strong> learning member models. The model learned in phase <span class="math notranslate nohighlight">\(i\)</span> has influence on the training of all following member models and of course on the overall ensemble model.</p></li>
</ul>
<p>For both categories algorithms for classication as well as regression exist.</p>
<div class="section" id="bagging">
<h2>Bagging<a class="headerlink" href="#bagging" title="Permalink to this headline">¶</a></h2>
<p>Bagging can be considered to be a <strong>parallel</strong> approach: <span class="math notranslate nohighlight">\(B\)</span> individual models are learned independent of each other. If all models are learned, the output of the ensemble is usually the average over the individual outputs:</p>
<div class="math notranslate nohighlight">
\[
f_{bag}(x)=\frac{1}{B}\sum\limits_{b=1}^B f_b(x)
\]</div>
<p>Here <span class="math notranslate nohighlight">\(x\)</span> is the given input (feature vector), <span class="math notranslate nohighlight">\(f_b(x)\)</span>  is the output of the <span class="math notranslate nohighlight">\(b\)</span>.th model and <span class="math notranslate nohighlight">\(f_{bag}\)</span> is the output of the ensemble. For bagging, the same learning algorithm (usually decision tree) is applied for all members of the ensemble. Diversity is provided by applying different training sets for each individual learner. The individual training sets are obtained by <strong>bootstraping</strong>: Let</p>
<div class="math notranslate nohighlight">
\[
T=\lbrace (x_1,r_1),(x_2,r_2),\ldots,(x_N,r_N) \rbrace
\]</div>
<p>be the set of <span class="math notranslate nohighlight">\(N\)</span> available training instances. For each individual learner randomly select a set of <span class="math notranslate nohighlight">\(N\)</span> samples with replacement out of <span class="math notranslate nohighlight">\(T\)</span>. Due to the selection with replacement the training set of a single learner may contain some samples more than once, whereas other samples from <span class="math notranslate nohighlight">\(T\)</span> are not included. There are about <span class="math notranslate nohighlight">\(N_S=\frac{2}{3}N\)</span> different elements in each bootstrap training set. The individual model will be adapted closer to the statistics of the training samples, that are contained  more than once, wheras the statistics of the not contained samples are disregarded.</p>
<p>One main benefit of bagging is that it reduces variance. The variance of a learner refers to it’s dependence on the training set. The variance is said to be high, if small changes in the training set yield large variations in the learned model and its output. Overfitted models usually have a large variance.</p>
<p>Popular representatives of Bagging Machine Learning algorithms</p>
<ul class="simple">
<li><p>Random Forests</p></li>
<li><p>Extremely Randomized Trees</p></li>
</ul>
<p>Both of them are based on randomized decision trees. I.e. each individual learner is a randomized decision tree and the overall model is an ensemble of such trees.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/bagging.png" style="width:600px" align="center"><div class="section" id="random-forest">
<h3>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<p>In scikit-learn <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier">RandomForestClassifier</a> and <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor">RandomForestRegressor</a> are implemented. Each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model. In contrast to the original publication, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class (cited from: <a class="reference external" href="http://scikit-learn.org/stable/modules/ensemble.html">http://scikit-learn.org/stable/modules/ensemble.html</a>).</p>
</div>
<div class="section" id="extremely-randomized-trees">
<h3>Extremely Randomized Trees<a class="headerlink" href="#extremely-randomized-trees" title="Permalink to this headline">¶</a></h3>
<p>In extremely randomized trees (see <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier">ExtraTreesClassifier</a> and <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor">ExtraTreesRegressor</a> classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias (cited from: <a class="reference external" href="http://scikit-learn.org/stable/modules/ensemble.html">http://scikit-learn.org/stable/modules/ensemble.html</a>).</p>
</div>
</div>
<div class="section" id="boosting">
<h2>Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">¶</a></h2>
<p>Boosting can be considered as a sequential approach. Usually a sequence of weak learners, each using the same learning algorithm, is combined such that each learner focuses on learning those patterns, which were unsufficiently processed by the previous learner. The weak classifiers must only be better than chance. The overall boosting algorithm is a linear combination of the individual weak classifiers.</p>
<div class="section" id="adaboost">
<h3>Adaboost<a class="headerlink" href="#adaboost" title="Permalink to this headline">¶</a></h3>
<p>Adaboost has been one of the first boosting algorithms. It is an ensemble classifier and it is still applied in a wide range of applications, e.g. in Face Detection. The idea of Adaboost is sketched in the figure below: In the first stage (leftmost picture) all training samples are weighted equally. A weak learner is trained with this training set. Training samples, which are misclassified by the first learner obtain a larger weight in the training set of the second learner. Thus the learner will be more adapted to these previously missclassified patterns. The patterns, which are missclassified by the second learner obtain a larger weight in the training set for the third learner and so on.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/boostingSchema.PNG" style="width:800px" align="center"><p>The final classifier <span class="math notranslate nohighlight">\(f_{boost}(x)\)</span> is a linear combination of the weak classifiers <span class="math notranslate nohighlight">\(f_m(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
f_{boost}(x)=\sum\limits_{m=1}^M \alpha_m f_m(x)
\]</div>
<p>Individual learners <span class="math notranslate nohighlight">\(f_m(x)\)</span> with a good performance contribute with a larger weight <span class="math notranslate nohighlight">\(\alpha_m\)</span> to the overall classifier, than weakly performing learners.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/adaboostAlg.JPG" style="width:600px" align="center"></div>
<div class="section" id="gradient-tree-boosting">
<h3>Gradient Tree Boosting<a class="headerlink" href="#gradient-tree-boosting" title="Permalink to this headline">¶</a></h3>
<p>Gradient Tree Boosting is a generalization of boosting to arbitrary differentiable loss functions. The base-learners are Regression Trees. In <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier">scikit-learn GradientBoostingClassifier</a> builds an additive model in a forward stage-wise fashion. It allows for the optimization of arbitrary differentiable loss functions. In each stage <em>n_classes_</em> regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. In <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor">scikit-learn GradientBoostingRegression</a> in each stage a regression tree is fit on the negative gradient of the given loss function.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="03ClassificationPipe.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Learn evaluate and compare Classification Models</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="05Optimisation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Hyperparameter Optimisation with Scikit-Learn</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      Durch Prof. Dr. Johannes Maucher<br/>
    
        &copy; Urheberrechte © 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>