{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Word Embeddings\n",
    "\n",
    "Word embeddings have revolutionalized many fields of Natural Language Processing (Semantic Analysis, Intelligent Search Engines, Document Classification, Machine Translation, ...) since their efficient neural-network-based generation has been published in [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) by Mikolov et al (2013). Word embeddings map words into vector-spaces such that semantically or syntactically related words are close together, whereas unrelated words are far from each other. \n",
    "\n",
    "## Task 1:\n",
    "\n",
    "Prepare yourself for this assignment by studying the following notebook:\n",
    "\n",
    "* [notebook 11ModellingWordsAndTexts.ipynb](../Lecture/11ModellingWordsAndTexts.ipynb). This notebook describes different methods to represent words and documents, in particular the representation of words by vectors (Word-Embedding). Calculation of Word-Embeddings with CBOW and Skipgram is explained. Moreover, the access of pretrained Word-Embeddings from e.g. Facebook's FastText project is explained. \n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. Documents can be represented by Bag-Of-Word (BoW). Describe the BoW-Model.\n",
    "2. What are the drawbacks of the BoW-model?\n",
    "3. What is a Word-Embedding?\n",
    "4. In the context of Word-Embeddings: Words are semantically similar if, ....?\n",
    "5. What is required to train a Word-Embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: \n",
    "1. From [Facebook's FastText project](https://fasttext.cc/docs/en/english-vectors.html) download the English Word-Embedding `wiki-news-300d-1M.vec.zip`.\n",
    "2. Load the embedding into your notebook by the corresponding gensim-module as described in [notebook 11ModellingWordsAndTexts.ipynb](../Lecture/11ModellingWordsAndTexts.ipynb). If you do so, the word-vectors are represented as an object of the `gensim`-class `KeyedVectors`. Implement at least 5 methods (such as `most_similar()`), which can be invoked for this class. For each of these methods shortly describe what they provide. Get help from here: [https://radimrehurek.com/gensim/models/keyedvectors.html](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **In the next assignment Word-Embeddings will be applied as input to deep neural networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
