

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Evolution of Textmodelling and -classification &#8212; Machine Learning (DSM)</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="../_static/togglebutton.js"></script>
    <script type="text/javascript" src="../_static/clipboard.min.js"></script>
    <script type="text/javascript" src="../_static/copybutton.js"></script>
    <script type="text/javascript" src="../_static/sphinx-book-theme.js"></script>
    <script type="text/javascript">var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" type="text/javascript" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script type="text/javascript">
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" type="text/javascript" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning (DSM)</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Overview.html">
   Machine Learning with Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning with Scikit-Learn
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02RegressionPipe.html">
   Learn and evaluate a Regression Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03ClassificationPipe.html">
   Learn evaluate and compare Classification Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05EnsembleMethods.html">
   Ensemble Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05Optimisation.html">
   Hyperparameter Optimisation with Scikit-Learn
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01NeuralNetworkImplementation.html">
   Multi Layer Perceptron for Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06KerasLSTMbikeRentalPrediction.html">
   LSTM Time Series Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="12FeatureSelection.html">
   Feature Selection and Extraction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Lecture/07DSM.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Lecture/07DSM.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/executablebooks/jupyter-book/blob/master/Lecture/07DSM.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            
        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Evolution-of-Textmodelling-and--classification" data-toc-modified-id="Evolution-of-Textmodelling-and--classification-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Evolution of Textmodelling and -classification</a></span></li><li><span><a href="#Word-Embeddings" data-toc-modified-id="Word-Embeddings-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Word Embeddings</a></span><ul class="toc-item"><li><span><a href="#Representations-of-single-Words" data-toc-modified-id="Representations-of-single-Words-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>Representations of single Words</a></span></li><li><span><a href="#Representations-of-Documents" data-toc-modified-id="Representations-of-Documents-2.2"><span class="toc-item-num">2.2&nbsp;&nbsp;</span>Representations of Documents</a></span><ul class="toc-item"><li><span><a href="#Bag-of-Word-model" data-toc-modified-id="Bag-of-Word-model-2.2.1"><span class="toc-item-num">2.2.1&nbsp;&nbsp;</span>Bag-of-Word model</a></span></li><li><span><a href="#Sequences-of-Word-Vectors" data-toc-modified-id="Sequences-of-Word-Vectors-2.2.2"><span class="toc-item-num">2.2.2&nbsp;&nbsp;</span>Sequences of Word-Vectors</a></span></li><li><span><a href="#Vector-Representations-of-Sentences,-Paragraphs-and-Documents" data-toc-modified-id="Vector-Representations-of-Sentences,-Paragraphs-and-Documents-2.2.3"><span class="toc-item-num">2.2.3&nbsp;&nbsp;</span>Vector-Representations of Sentences, Paragraphs and Documents</a></span></li></ul></li><li><span><a href="#CBOW--and-Skipgram--Wordembedding" data-toc-modified-id="CBOW--and-Skipgram--Wordembedding-2.3"><span class="toc-item-num">2.3&nbsp;&nbsp;</span>CBOW- and Skipgram- Wordembedding</a></span><ul class="toc-item"><li><span><a href="#Continous-Bag-Of-Words-(CBOW)" data-toc-modified-id="Continous-Bag-Of-Words-(CBOW)-2.3.1"><span class="toc-item-num">2.3.1&nbsp;&nbsp;</span>Continous Bag-Of-Words (CBOW)</a></span></li><li><span><a href="#Skip-Gram" data-toc-modified-id="Skip-Gram-2.3.2"><span class="toc-item-num">2.3.2&nbsp;&nbsp;</span>Skip-Gram</a></span></li></ul></li><li><span><a href="#Other-Word-Embeddings" data-toc-modified-id="Other-Word-Embeddings-2.4"><span class="toc-item-num">2.4&nbsp;&nbsp;</span>Other Word-Embeddings</a></span><ul class="toc-item"><li><span><a href="#Integration-of-Fasttext-Word-Embeddings" data-toc-modified-id="Integration-of-Fasttext-Word-Embeddings-2.4.1"><span class="toc-item-num">2.4.1&nbsp;&nbsp;</span>Integration of Fasttext Word-Embeddings</a></span></li><li><span><a href="#Glove-Word-Embeddings" data-toc-modified-id="Glove-Word-Embeddings-2.4.2"><span class="toc-item-num">2.4.2&nbsp;&nbsp;</span>Glove Word-Embeddings</a></span></li></ul></li><li><span><a href="#Examples-of-Semantic-Relatedness-from-Word-Embeddings" data-toc-modified-id="Examples-of-Semantic-Relatedness-from-Word-Embeddings-2.5"><span class="toc-item-num">2.5&nbsp;&nbsp;</span>Examples of Semantic Relatedness from Word-Embeddings</a></span><ul class="toc-item"><li><span><a href="#Visualisation-through-t-SNE-projection-in-2-dim-space" data-toc-modified-id="Visualisation-through-t-SNE-projection-in-2-dim-space-2.5.1"><span class="toc-item-num">2.5.1&nbsp;&nbsp;</span>Visualisation through t-SNE projection in 2-dim space</a></span></li><li><span><a href="#Find-most-similar-words" data-toc-modified-id="Find-most-similar-words-2.5.2"><span class="toc-item-num">2.5.2&nbsp;&nbsp;</span>Find most similar words</a></span></li><li><span><a href="#Vector-representations-of-semantic-relations" data-toc-modified-id="Vector-representations-of-semantic-relations-2.5.3"><span class="toc-item-num">2.5.3&nbsp;&nbsp;</span>Vector representations of semantic relations</a></span></li><li><span><a href="#Lexical-Contrast-Injection" data-toc-modified-id="Lexical-Contrast-Injection-2.5.4"><span class="toc-item-num">2.5.4&nbsp;&nbsp;</span>Lexical Contrast Injection</a></span></li><li><span><a href="#Multilingual-Embeddings" data-toc-modified-id="Multilingual-Embeddings-2.5.5"><span class="toc-item-num">2.5.5&nbsp;&nbsp;</span>Multilingual Embeddings</a></span></li></ul></li></ul></li></ul></div><p><a class="reference internal" href="Overview.html"><span class="doc std std-doc">Go to Workshop Overview (.ipynb)</span></a></p>
<div class="section" id="evolution-of-textmodelling-and-classification">
<h1>Evolution of Textmodelling and -classification<a class="headerlink" href="#evolution-of-textmodelling-and-classification" title="Permalink to this headline">¶</a></h1>
<p><img alt="NLP Overall Picture" src="Lecture/./Pics/overAllPicture.png" /></p>
</div>
<div class="section" id="word-embeddings">
<h1>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Author: Johannes Maucher</p></li>
<li><p>Last Update: 31.07.2018</p></li>
</ul>
<div class="section" id="representations-of-single-words">
<h2>Representations of single Words<a class="headerlink" href="#representations-of-single-words" title="Permalink to this headline">¶</a></h2>
<p>There are different ways to represent single words at the input of a machine learning algorithm. The conventional (old) way is to apply a <strong>one-hot-encoding</strong>. In this approach each word is represented by a vector <span class="math notranslate nohighlight">\(\mathbf{v_w}\)</span>, whose length is given by the number of words in the applied vocabulary <span class="math notranslate nohighlight">\(V\)</span>. Each word <span class="math notranslate nohighlight">\(w \in V\)</span> is uniquely mapped to an integer <span class="math notranslate nohighlight">\(i_w \in [0,|V|]\)</span> and the one-hot encoded word-vector <span class="math notranslate nohighlight">\(\mathbf{v_w}\)</span> contains a 1 at position <span class="math notranslate nohighlight">\(i_w\)</span> and zeros at all other positions. Drawbacks of the one-hot representation of words are:</p>
<ul class="simple">
<li><p>the one-hot encoded vectors are very long and sparse</p></li>
<li><p>semantic or syntactic relations between words, can not be infered from the corresponding one-hot encoded vectors.</p></li>
</ul>
<p>The better way of representing words is to apply a <strong>word-embedding</strong>. Word embeddings have revolutionalized many fields of Natural Language Processing since their efficient neural-network-based generation has been published in <a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> by Mikolov et al (2013). Word embeddings map words into vector-spaces such that semantically or syntactically related words are close together, whereas unrelated words are far from each other. Moreover, it has been shown that the word-embeddings, generated by <em>word2vec</em>-techniques <em>CBOW</em> or <em>Skipgram</em>, are well-structured in the sense that also relations such as <em>is-capital-of</em>, <em>is-female-of</em>, <em>is-plural-of</em> are encoded in the vector space. In this way questions like <em>woman is to queen, as man is to ?</em> can be answered by simple operations of linear algebra in the word-vector-space. Compared to the length of one-hot encoded word-vectors, word-embedding-vectors are short (typical lengths in the range from 100-300) and dense (float-values).</p>
<p><img alt="DSM concept" src="Lecture/./Pics/dsm.png" /></p>
<p><em>CBOW</em> and <em>Skipgram</em>, are techniques to learn word-embeddings, i.e. a mapping of words to vectors by relatively simple neural networks. Usually large corpora are applied for learning, e.g. the entire Wikipedia corpus in a given language. Today, pretrained models for the most common languages are available, for example from this <a class="reference external" href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md?utm_campaign=buffer&amp;utm_content=buffer0df9b&amp;utm_medium=social&amp;utm_source=linkedin.com">Facebook github repository</a>.</p>
<p>For the classifier implemented in <span class="xref myst">notebook K04germanNewsFeedClassification.ipynb</span>, a self-trained german word-embedding is applied. The embedding has been trained with CBOW from the dump of the entire german Wikipedia in notebook <span class="xref myst">K03generateCBOWfromWiki.ipynb</span></p>
</div>
<div class="section" id="representations-of-documents">
<h2>Representations of Documents<a class="headerlink" href="#representations-of-documents" title="Permalink to this headline">¶</a></h2>
<div class="section" id="bag-of-word-model">
<h3>Bag-of-Word model<a class="headerlink" href="#bag-of-word-model" title="Permalink to this headline">¶</a></h3>
<p>The conventional way of modelling documents in tasks like information-retrieval, document-clustering, document-classification, sentiment-analysis, topic-classification is to represent each document as a <strong>Bag-Of-Word</strong>-vector $<span class="math notranslate nohighlight">\(\mathbf{d}_i=(tf_{i,0},tf_{i,1},\ldots tf_{i,|V|})\)</span><span class="math notranslate nohighlight">\(. Each component of this vector corresponds to a single term \)</span>j<span class="math notranslate nohighlight">\( of the underlying vocabulary \)</span>V<span class="math notranslate nohighlight">\( and the values \)</span>tf_{i,j}<span class="math notranslate nohighlight">\( counts the frequency of term \)</span>j<span class="math notranslate nohighlight">\( in document \)</span>i<span class="math notranslate nohighlight">\(. Instead of the term-frequency \)</span>tf_{i,j}$ it is also possible to fill the BoW-vector with</p>
<ul class="simple">
<li><p>a binary indicator which indicates if the term <span class="math notranslate nohighlight">\(j\)</span> appears in document <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p>the tf-idf-values $<span class="math notranslate nohighlight">\(tfidf_{i,j}=tf_{i,j} \cdot log \frac{N}{df_j},\)</span><span class="math notranslate nohighlight">\( where \)</span>df_j<span class="math notranslate nohighlight">\( is the frequency of documents, in which term \)</span>j<span class="math notranslate nohighlight">\( appears, and \)</span>N$ is the total number of documents.</p></li>
</ul>
<p>Independent of the values used, the BoW-model has the following major drawbacks:</p>
<ul class="simple">
<li><p>the order by which terms appear in the document is totally ignored</p></li>
<li><p>semantic relatedness of terms is not modelled</p></li>
<li><p>BoW- vectors are very long and sparse</p></li>
</ul>
</div>
<div class="section" id="sequences-of-word-vectors">
<h3>Sequences of Word-Vectors<a class="headerlink" href="#sequences-of-word-vectors" title="Permalink to this headline">¶</a></h3>
<p>Using word-embeddings all of the mentioned drawbacks of BoW-modells can be avoided. One approach is to represent the sequence of words in the document as a sequence of word-embedding vectors. These sequences can then be passed to e.g. Convolutional Neural Networks (CNN) or Long-Short-Term-Memory Networks (LSTM).</p>
<p><img alt="BoWvsEmbedding" src="Lecture/./Pics/bowVsEmbedding.png" /></p>
</div>
<div class="section" id="vector-representations-of-sentences-paragraphs-and-documents">
<h3>Vector-Representations of Sentences, Paragraphs and Documents<a class="headerlink" href="#vector-representations-of-sentences-paragraphs-and-documents" title="Permalink to this headline">¶</a></h3>
<p>Word-Embeddings are based on the idea, that semantically related words can be mapped to vectors, which are close together. Where two words are semantically related, if they frequently appear in the same context (neighbouring words). This idea can also be applied to sentences or word-sequences in general. By applying e.g. LSTMs, Doc2Vec, Encoder-Decoder architectures etc. vector representations of texts can be generated, such that semantically related texts have similiar vector-representations.</p>
</div>
</div>
<div class="section" id="cbow-and-skipgram-wordembedding">
<h2>CBOW- and Skipgram- Wordembedding<a class="headerlink" href="#cbow-and-skipgram-wordembedding" title="Permalink to this headline">¶</a></h2>
<p>In 2013 Mikolov et al. published <a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a>. They proposed quite simple neural network architectures to efficiently create word-embeddings: CBOW and Skipgram. These architectures are better known as <strong>Word2Vec</strong>. In both techniques neural networks are trained for a pseudo-task. After training, the network itself is usually not of interest. However, the learned weights in the input-layer constitute the word-embeddings, which can then be applied for a large field of NLP-tasks, e.g. document classification.</p>
<div class="section" id="continous-bag-of-words-cbow">
<h3>Continous Bag-Of-Words (CBOW)<a class="headerlink" href="#continous-bag-of-words-cbow" title="Permalink to this headline">¶</a></h3>
<p>The idea of CBOW is to predict the target word <span class="math notranslate nohighlight">\(w_i\)</span>, given the <span class="math notranslate nohighlight">\(N\)</span> context-words <span class="math notranslate nohighlight">\(w_{i-N/2},\ldots, w_{i-1}, \quad w_{i+1}, w_{i+N/2}\)</span>.
In order to learn such a predictor a large but unlabeled corpus is required. The extraction of training-samples from a corpus is sketched in the picture below:</p>
<p><img alt="cbowTrainSamples" src="Lecture/./Pics/cbowTrainSamples.png" /></p>
<p>In this example a context length of <span class="math notranslate nohighlight">\(N=4\)</span> has been applied. The first training-element consists of</p>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(N=4\)</span> input-words <em>(happy,families,all,alike)</em></p></li>
<li><p>the target word <em>are</em>.</p></li>
</ul>
<p>In order to obtain the second training-sample the window of length <span class="math notranslate nohighlight">\(N+1\)</span> is just shifted by one to the right. The concrete architecture for CBOW is shown in the picture below. At the input the <span class="math notranslate nohighlight">\(N\)</span> context words are one-hot-encoded. The fully-connected <em>Projection-layer</em> maps the context words to a vector representation of the context. This vector representation is the input of a softmax-output-layer. The output-layer has as much neurons as there are words in the vocabulary <span class="math notranslate nohighlight">\(V\)</span>. Each neuron uniquely corresponds to a word of the vocabulary and outputs an estimation of the probaility, that the word appears as target for the current context-words at the input.<br />
<img alt="cbowArchitecture" src="Lecture/./Pics/cbowGramArchitecture.png" /></p>
<p>Once the CBOW-network is trained, the vector representation of word <span class="math notranslate nohighlight">\(w\)</span> are the weights from the one-hot encoded word <span class="math notranslate nohighlight">\(w\)</span> at the input of the network to the neurons in the projection-layer. I.e. the number of neurons in the projection layer define the length of the word-embedding.</p>
</div>
<div class="section" id="skip-gram">
<h3>Skip-Gram<a class="headerlink" href="#skip-gram" title="Permalink to this headline">¶</a></h3>
<p>Skip-Gram is similar to CBOW, but has a reversed prediction process: For a given target word at the input, the Skip-Gram model predicts words, which are likely in the context of this target word. Again, the context is defined by the <span class="math notranslate nohighlight">\(N\)</span> neighbouring words. The extraction of training-samples from a corpus is sketched in the picture below:</p>
<p><img alt="skipGramTrainSamples" src="Lecture/./Pics/skipGramTrainSamples.png" /></p>
<p>Again a context length of <span class="math notranslate nohighlight">\(N=4\)</span> has been applied. The first training-element consists of</p>
<ul class="simple">
<li><p>the first target word <em>(happy)</em> as input to the network</p></li>
<li><p>the first context word <em>(families)</em> as network-output.</p></li>
</ul>
<p>The concrete architecture for Skip-gram is shown in the picture below. At the input the target-word is one-hot-encoded. The fully-connected <em>Projection-layer</em> outputs the current vector representation of the target-word. This vector representation is the input of a softmax-output-layer. The output-layer has as much neurons as there are words in the vocabulary <span class="math notranslate nohighlight">\(V\)</span>. Each neurons uniquely corresponds to a word of the vocabulary and outputs an estimation of the probaility, that the word appears in the context of the current target-word at the input.<br />
<img alt="cbowArchitecture" src="Lecture/./Pics/skipGramArchitecture.png" /></p>
</div>
</div>
<div class="section" id="other-word-embeddings">
<h2>Other Word-Embeddings<a class="headerlink" href="#other-word-embeddings" title="Permalink to this headline">¶</a></h2>
<p>CBOW- and Skip-Gram are possibly the most popular word-embeddings. However, there are more count-based and prediction-based methods to generate them, e.g. Random-Indexing, <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">Glove</a>, <a class="reference external" href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md">FastText</a>.</p>
<div class="section" id="integration-of-fasttext-word-embeddings">
<h3>Integration of Fasttext Word-Embeddings<a class="headerlink" href="#integration-of-fasttext-word-embeddings" title="Permalink to this headline">¶</a></h3>
<p>After downloading word embeddings from <a class="reference external" href="https://fasttext.cc/docs/en/english-vectors.html">FastText</a> they can be imported as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
<span class="c1"># Creating the model</span>
<span class="n">pathMacBookAir</span><span class="o">=</span><span class="s1">&#39;/Users/johannesmaucher/DataSets/fasttextEnglish300.vec&#39;</span>
<span class="n">pathMacBook</span><span class="o">=</span><span class="s1">&#39;/Users/maucher/DataSets/Gensim/FastText/fasttextEnglish300.vec&#39;</span>
<span class="n">pathDeepLearn</span><span class="o">=</span><span class="s1">&#39;../../DataSets/FastText/fasttextEnglish300.vec&#39;</span>
<span class="n">w2vmodel</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">pathMacBook</span><span class="p">)</span>
<span class="n">EMBEDDING_DIM</span><span class="o">=</span><span class="n">w2vmodel</span><span class="o">.</span><span class="n">vector_size</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">cb742a87c4d1</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">pathMacBook</span><span class="o">=</span><span class="s1">&#39;/Users/maucher/DataSets/Gensim/FastText/fasttextEnglish300.vec&#39;</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">pathDeepLearn</span><span class="o">=</span><span class="s1">&#39;../../DataSets/FastText/fasttextEnglish300.vec&#39;</span>
<span class="ne">----&gt; </span><span class="mi">6</span> <span class="n">w2vmodel</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">pathMacBook</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">EMBEDDING_DIM</span><span class="o">=</span><span class="n">w2vmodel</span><span class="o">.</span><span class="n">vector_size</span>

<span class="nn">~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/gensim/models/keyedvectors.py</span> in <span class="ni">load_word2vec_format</span><span class="nt">(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)</span>
<span class="g g-Whitespace">   </span><span class="mi">1496</span>         <span class="k">return</span> <span class="n">_load_word2vec_format</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1497</span>             <span class="bp">cls</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">fvocab</span><span class="o">=</span><span class="n">fvocab</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="n">binary</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="n">unicode_errors</span><span class="p">,</span>
<span class="ne">-&gt; </span><span class="mi">1498</span>             <span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">,</span> <span class="n">datatype</span><span class="o">=</span><span class="n">datatype</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1499</span> 
<span class="g g-Whitespace">   </span><span class="mi">1500</span>     <span class="k">def</span> <span class="nf">get_keras_embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_embeddings</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

<span class="nn">~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/gensim/models/utils_any2vec.py</span> in <span class="ni">_load_word2vec_format</span><span class="nt">(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)</span>
<span class="g g-Whitespace">    </span><span class="mi">340</span> 
<span class="g g-Whitespace">    </span><span class="mi">341</span>     <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading projection weights from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">342</span>     <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">343</span>         <span class="n">header</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">fin</span><span class="o">.</span><span class="n">readline</span><span class="p">(),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">344</span>         <span class="n">vocab_size</span><span class="p">,</span> <span class="n">vector_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">header</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>  <span class="c1"># throws for invalid file format</span>

<span class="nn">~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/smart_open/smart_open_lib.py</span> in <span class="ni">open</span><span class="nt">(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)</span>
<span class="g g-Whitespace">    </span><span class="mi">306</span>         <span class="n">buffering</span><span class="o">=</span><span class="n">buffering</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">307</span>         <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span>
<span class="ne">--&gt; </span><span class="mi">308</span>         <span class="n">errors</span><span class="o">=</span><span class="n">errors</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">309</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">310</span>     <span class="k">if</span> <span class="n">fobj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

<span class="nn">~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/smart_open/smart_open_lib.py</span> in <span class="ni">_shortcut_open</span><span class="nt">(uri, mode, ignore_ext, buffering, encoding, errors)</span>
<span class="g g-Whitespace">    </span><span class="mi">515</span>     <span class="c1">#</span>
<span class="g g-Whitespace">    </span><span class="mi">516</span>     <span class="k">if</span> <span class="n">six</span><span class="o">.</span><span class="n">PY3</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">517</span>         <span class="k">return</span> <span class="n">_builtin_open</span><span class="p">(</span><span class="n">parsed_uri</span><span class="o">.</span><span class="n">uri_path</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">buffering</span><span class="o">=</span><span class="n">buffering</span><span class="p">,</span> <span class="o">**</span><span class="n">open_kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">518</span>     <span class="k">elif</span> <span class="ow">not</span> <span class="n">open_kwargs</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">519</span>         <span class="k">return</span> <span class="n">_builtin_open</span><span class="p">(</span><span class="n">parsed_uri</span><span class="o">.</span><span class="n">uri_path</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">buffering</span><span class="o">=</span><span class="n">buffering</span><span class="p">)</span>

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;/Users/maucher/DataSets/Gensim/FastText/fasttextEnglish300.vec&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Printing out number of tokens available</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of Tokens: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w2vmodel</span><span class="o">.</span><span class="n">index2word</span><span class="p">)))</span>
<span class="c1"># Printing out the dimension of a word vector </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dimension of a word vector: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">EMBEDDING_DIM</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of Tokens: 1000000
Dimension of a word vector: 300
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">=</span><span class="s2">&quot;than&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First 10 components of word-vector of word </span><span class="si">{}</span><span class="s2">: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">),</span><span class="n">w2vmodel</span><span class="p">[</span><span class="n">w</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First 10 components of word-vector of word than: 
 [ 0.1016 -0.1216 -0.0356  0.0096 -0.1015  0.1766 -0.0593  0.032   0.0892
 -0.0727]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="glove-word-embeddings">
<h3>Glove Word-Embeddings<a class="headerlink" href="#glove-word-embeddings" title="Permalink to this headline">¶</a></h3>
<p>After downloading word-embeddings from <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">Glove</a>, they can be imported as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">w2vmodel</span><span class="o">=</span><span class="p">{}</span>
<span class="n">GLOVE_DIR</span> <span class="o">=</span> <span class="s2">&quot;/Users/maucher/DataSets/glove.6B&quot;</span>
<span class="c1">#GLOVE_DIR = &quot;./Data/glove.6B&quot;</span>
<span class="c1">#embeddings_index = {}</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">GLOVE_DIR</span><span class="p">,</span> <span class="s1">&#39;glove.6B.100d.txt&#39;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
    <span class="n">w2vmodel</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">=</span><span class="n">coefs</span>
    <span class="c1">#embeddings_index[word] = coefs</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">EMBEDDING_DIM</span><span class="o">=</span><span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total </span><span class="si">%s</span><span class="s1"> word vectors in Glove 6B 100d.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">w2vmodel</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total 400000 word vectors in Glove 6B 100d.
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="examples-of-semantic-relatedness-from-word-embeddings">
<h2>Examples of Semantic Relatedness from Word-Embeddings<a class="headerlink" href="#examples-of-semantic-relatedness-from-word-embeddings" title="Permalink to this headline">¶</a></h2>
<div class="section" id="visualisation-through-t-sne-projection-in-2-dim-space">
<h3>Visualisation through t-SNE projection in 2-dim space<a class="headerlink" href="#visualisation-through-t-sne-projection-in-2-dim-space" title="Permalink to this headline">¶</a></h3>
<p><img alt="SemanticRelatednessVis" src="Lecture/./Pics/semanticRelatednessVis.png" /></p>
<p>Source: <a class="reference external" href="http://metaoptimize.s3.amazonaws.com/cw-embeddings-ACL2010/embeddings-mostcommon.EMBEDDING_SIZE=50.png">http://metaoptimize.s3.amazonaws.com/cw-embeddings-ACL2010/embeddings-mostcommon.EMBEDDING_SIZE=50.png</a></p>
</div>
<div class="section" id="find-most-similar-words">
<h3>Find most similar words<a class="headerlink" href="#find-most-similar-words" title="Permalink to this headline">¶</a></h3>
<p><img alt="collobertSimilarities" src="Lecture/./Pics/collobertSimilarities.png" />.</p>
<p>Source: <a class="reference external" href="https://arxiv.org/pdf/1103.0398v1.pdf">R. Collobert, NLP (almost) from Scratch</a></p>
</div>
<div class="section" id="vector-representations-of-semantic-relations">
<h3>Vector representations of semantic relations<a class="headerlink" href="#vector-representations-of-semantic-relations" title="Permalink to this headline">¶</a></h3>
<p><img alt="semanticRelations" src="Lecture/./Pics/semanticRelations.png" /></p>
<p>Source: <a class="reference external" href="https://www.aclweb.org/anthology/N/N13/N13-1090.pdf">T. Mikolov et al, Linguistic Regularities in Continuous Space Word Representations</a></p>
</div>
<div class="section" id="lexical-contrast-injection">
<h3>Lexical Contrast Injection<a class="headerlink" href="#lexical-contrast-injection" title="Permalink to this headline">¶</a></h3>
<p><img alt="lexicalContrast" src="Lecture/./Pics/lexicalContrastInjection.png" />
Source: <a class="reference external" href="https://www.aclweb.org/anthology/P15-2004">Pham et al, A Multitask Objective to Inject Lexical Contrast into Distributional Semantics</a></p>
</div>
<div class="section" id="multilingual-embeddings">
<h3>Multilingual Embeddings<a class="headerlink" href="#multilingual-embeddings" title="Permalink to this headline">¶</a></h3>
<p><img alt="Bilingual Embedding" src="Lecture/./Pics/bilingualEmbedding.png" />
Source: <a class="reference external" href="https://arxiv.org/pdf/1706.04902.pdf">Ruder et al, A Survey of Cross-lingual Word Embedding Models</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>