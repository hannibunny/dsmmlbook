{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python\n",
    "\n",
    "* Author: Prof. Dr. Johannes Maucher\n",
    "* Email: maucher@hdm-stuttgart.de\n",
    "* Last Update: December, 1st 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals of this lecture are:**\n",
    "\n",
    "* Understand all steps of the datamining process, from data access to visualisation and interpretation of the results.\n",
    "* Learn how to implement all these process steps in Python, applying libraries like [Pandas](http://pandas.pydata.org/), [NumPy](http://www.numpy.org/), [Matplotlib](http://matplotlib.org/), [Scikit-Learn](http://scikit-learn.org/stable/index.html) etc.\n",
    "* Understand Machine Learning algorithms, in particular for supervised learning.\n",
    "* Learn how to integrate these algorithms from scikit-learn into datamining projects.\n",
    "* Understand Neural Networks, in particular Deep Neural Networks\n",
    "* Learn how to implement neural network- and deep neural network applications with [Keras](https://keras.io/).\n",
    "* Understand how to model words and documents for textmining\n",
    "* Learn how to implement methods for topic extraction and text classification\n",
    "\n",
    "**Teaching form:** \n",
    "\n",
    "* Contents of this course will be tought task-driven. This means that we start from the tasks of the [Live coding exercise-notebooks](#livecoding). In order to solve these tasks theory and programming skills are required from the notebooks in chapter [Data Mining with Python](#data_mining). We will consult these notebooks whenever we require the corresponding skills to solve the tasks of the exercise notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Organisation of Jupyter Notebooks for Lecture, Live-Coding and Assignments**\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/MachineLearningLearningPath.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basic_modules'></a>\n",
    "## Prerequisites\n",
    "This course requires basic knowledge in Python and\n",
    "\n",
    "* [Basics in Numpy (.ipynb)](NP01numpyBasics.ipynb) \n",
    "* [Basics in Matplotlib (.ipynb)](PLT01visualization.ipynb) \n",
    "* [Basics in Pandas (.ipynb)](PD01Pandas.ipynb)\n",
    "\n",
    "Moreover, the Python Machine Learning framework [Scikit-Learn](http://scikit-learn.org/stable/index.html) will be extensively applied in this course. The main concepts of this library are:\n",
    "\n",
    "* it is primarily built on Numpy. In particular internal and external data structures are [Numpy Arrays](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html).\n",
    "* All algorithms, which somehow transform data belong to the **Transformer**-class, e.g. *PCA, Normalizer, StandardScaler, OneHotEncoder*, etc. These transformers are trained by applying the *.fit(traindata)*-method. Once they are trained, there *.transform(data)*-method can be called in order to transform *data*. If the data used for training the transformer shall be transformed immediately after training, the *.fit_transform(data)*-method can be applied.\n",
    "* All Machine Learning algorithms for supervised and unsupervised learning belong to the **Estimator** class, e.g. *LogisticRegression, SVM, MLP, Kmeans*, etc. These estimators are trained by applying the *.fit(trainfeatures)*- or *.fit(trainfeatures,trainlabels)*-method. The former configuration is applied for unsupervised-, the latter for supervised learning. Once an estimator is trained, it can be applied for clustering, classification or regression by envoking the *.predict(data)*-method. \n",
    "* At their interfaces all **Transformers** and **Estimators** apply *Numpy Arrays*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_mining'></a>\n",
    "## Machine Learning\n",
    "\n",
    "### Conventional techniques with scikit-learn\n",
    "\n",
    "1. [Basic Concepts of Data Mining and Machine Learning](00BasicConcepts.ipynb)\n",
    "    * Definition\n",
    "    * Categories\n",
    "    * Validation\n",
    "    \n",
    "2. [Regression Model](02RegressionPipe.ipynb)\n",
    "    * Example Data: Insurance Data\n",
    "    * Entire Data Mining process from data access to evaluation  \n",
    "    * One-Hot-Encoding\n",
    "    * Scaling\n",
    "    * Learn and apply Regression Model\n",
    "    * Evaluation Metrics for Regression Models\n",
    "    \n",
    "3. [Classification Model](03ClassificationPipe.ipynb) \n",
    "    * Example Data: Cleveland Heart Disease Dataset\n",
    "    * Cleaning, One-Hot-Encoding\n",
    "    * Building a pipeline of modules for scaling, transformation and classification\n",
    "    * Evaluation of a Classifier by accuracy, confusion matrix, precision, recall, f1-score\n",
    "    * Cross-Validation\n",
    "    * Determine feature importance\n",
    "    * Fast and efficient model comparison\n",
    "    \n",
    "5. [Ensemble Methods: General Concept](05EnsembleMethods.ipynb) \n",
    "    * Categorisation of ensemble machine learning algorithms and the main concepts\n",
    "    \n",
    "5. [Hyperparameter Optimisation](05Optimisation.ipynb)  \n",
    "    * Example Data: Predict bike rental\n",
    "    * Train and evaluate Random Forest Regression model\n",
    "    * Error visualisation\n",
    "    * Determining feature importance\n",
    "    * Hyperparameter Tuning\n",
    "    * Fast and efficient model comparison\n",
    "    * Comparison with Extremly Randomized Trees\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "2. [Modelling of Words and Texts](11ModellingWordsAndTexts.ipynb)\n",
    "    * One-Hot-Encoding of Words\n",
    "    * BoW model of texts\n",
    "    \n",
    "    \n",
    "\n",
    "2. [Feature Selection and Extraction(.ipynb)](02FeatureSelection.ipynb)\n",
    "    * Univariate Feature Selection Tests\n",
    "        * Entropy\n",
    "        * Mutual Information\n",
    "        * $\\chi^2$-Test\n",
    "        * F-Measure\n",
    "    * Variance Threshold\n",
    "    * Feature Selectors in scikitlearn: SelectKBest and SelectPercentile\n",
    "    * Principal Component Analysis (PCA)\n",
    "    * Linear Discriminant Analysis (LDA)\n",
    "\n",
    "\n",
    "\n",
    "6. [Clustering Energy Consumption (.ipynb)](06ClusteringEnergy.ipynb) \n",
    "    * Boxplots\n",
    "    * Enhance data with geo-information\n",
    "    * Normalization\n",
    "    * Clusteralgorithms\n",
    "        - Hierarchical Clustering\n",
    "        - Kmeans\n",
    "        - DBSAN\n",
    "        - Affinity Propagation\n",
    "    * Visualisation of clusters\n",
    "    * Dimensionality Reduction\n",
    "    * Visualisation in Google Maps\n",
    "    \n",
    "### Neural Networks and Deep Neural Networks\n",
    "\n",
    "11. [Neural Networks I: Single Layer Perceptron (SLP)(.ipynb)](SLP.ipynb) \n",
    "    * Natural Neuron\n",
    "    * General Notions for Artificial Neural Networks\n",
    "    * Single Layer Perceptron (SLP)\n",
    "        * Architectures for Regression and Classification\n",
    "        * Gradient Descent- and Stochastic Gradient Descent Learning\n",
    "    * SLP implementation and demonstration in Python\n",
    "    * SLP in Scikit-Learn: Implementation, Evaluation, Optimisation\n",
    "\n",
    "8. [Neural Networks II: Multi Layer Perceptron (MLP)(.ipynb)](MLP.ipynb)\n",
    "    * MLP Architectures for Regression and Classification\n",
    "    * Gradient Descent- and Stochastic Gradient Descent Learning\n",
    "    * MLP implementation and demonstration in Python\n",
    "    * MLP in Scikit-Learn: Implementation, Evaluation, Optimisation, \n",
    "    * Grid Search and Random Search for Hyperparameter-Tuning\n",
    "\n",
    "9. [Deep Neural Networks: Convolutional Neural Networks(.ipynb)](ConvolutionNeuralNetworks.ipynb) \n",
    "    * Overall Architecture of CNNs\n",
    "    * General concept of convolution filtering\n",
    "    * Layer-types: \n",
    "        * Convolution, \n",
    "        * Pooling, \n",
    "        * Fully-Connected\n",
    "\n",
    "10. [Recurrent Neural Networks (RNN)](RecurrentNeuralNetworks.ipynb) \n",
    "    * Simple Recurrent Neural Networks (RNNs)\n",
    "    * Long short-term Memory Networks (LSTMs)\n",
    "    * Gated Recurrent Units (GRUs)\n",
    "    * Application Categories of Recurrent Networks\n",
    "\n",
    "10. [Time-Series Prediction with Recurrent Neural Networks (LSTM) - Prediction of Bike rentals](LSTMbikeRentalPrediction.ipynb)\n",
    "    * Data Visualisation for Time-Series\n",
    "    * Building Recurrent Neural Networks with Keras\n",
    "    * RNN in many-to-one architecture\n",
    "\n",
    "10. [Time-Series Prediction with Recurrent Neural Networks (LSTM) - Temperature Prediction](LSTMtemperature.ipynb) \n",
    "    * Building Recurrent Neural Networks with Keras\n",
    "    * RNN in many-to-one architecture\n",
    "    * Weather-Forecasting with MLP, LSTM, GRU\n",
    "\n",
    "10. [Time-Series Prediction with Recurrent Neural Networks (LSTM) - Stock-Price-Prediction](LSTMstockPricePrediction.ipynb)\n",
    "    * Building Recurrent Neural Networks with Keras\n",
    "    * RNN in many-to-one architecture\n",
    "    * In notebook [LSTM Sequence Modelling on Stock-Price Data](LSTMsequenceModelStock.ipynb) a RNN is applied in many-to-many mode for the same task.\n",
    "\n",
    "### Define MLP and CNN for Object Classification in Keras\n",
    "\n",
    "18. [MLP and CNN for Object Classification](K03KerasMLPandCNNcifar.ipynb)\n",
    "\n",
    "### Apply pretrained Deep Neural Networks\n",
    "\n",
    "19. [Use of pretrained CNNs for object classification - original task](K04KerasPretrainedClassifiers.ipynb)\n",
    "20. [Use of pretrained CNNs for object classification - new task: Malaria detection](K04KerasPretrainedMalariaDetection.ipynb)\n",
    "\n",
    "### Textprocessing-, analysis and -classification\n",
    "\n",
    "21. [Access RSS Feeds and create corpus](T01crawlRSSFeeds.ipynb) \n",
    "    * Crawling of RSS feeds\n",
    "    * Generate a corresponding RSS feed news corpus\n",
    "    * *Note:* This notebook just demonstrates how data, which will be applied in the document-classification notebooks, has been collected. \n",
    "\n",
    "12. [Bag-of-Word Document Model, Similarity, Topic Extraction](T02topicExtractionRSSfeeds.ipynb) \n",
    "    * Access Data from the RSS news corpus (as generated in the previous notebook)\n",
    "    * Visualization of wordclouds\n",
    "    * Gensim\n",
    "    * Calculating document similarity\n",
    "    * LSI Topic Extraction\n",
    "    * LDA Topic Extraction\n",
    "\n",
    "13. [Word Embeddings Theory:](T03DSM.ipynb) \n",
    "    * Concept of Word-Embeddings\n",
    "    * Skip-Gram and CBOW\n",
    "    * Working with pretrained word-embeddings\n",
    "\n",
    "13. [Word Embeddings:](T03generateCBOWfromWiki.ipynb)\n",
    "    * Generate Word Embedding from German Wikipedia Dump\n",
    "    * Gensim Semantic Similarity analysis based on word-embeddings.\n",
    "\n",
    "14. [Text Classification with CNNs and LSTMs](T04germanNewsFeedClassification.ipynb) \n",
    "    * Text preprocessing and representation with Keras\n",
    "    * Load and apply pretrained word-embedding\n",
    "    * News classification with CNN\n",
    "    * News classification with LSTM\n",
    "\n",
    "15. [Classification of IMDB Movie Reviews with CNNs](T05CNN.ipynb) \n",
    "    * Sentiment analysis on IMDB reviews\n",
    "    * Load and apply different pretrained word-embeddings\n",
    "    * Implement and evaluate different CNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mlfow\n",
    "\n",
    "1. [Logging a scikit-learn model in mlflow](mlflowSklearnSmoker.ipynb)\n",
    "2. [Logging a keras model in mlflow](mlflowKerasReutersClassification.ipynb)\n",
    "3. [Analyse mlflow models in jupyter notebook](mlflowAnalyseModels.ipynb)\n",
    "\n",
    "You may also analyse your mlflow models by typing `mlflow ui` into your console from the directory, which contains your mlflow-directory. Then a local server starts under `http://127.0.0.1:5000`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='livecoding'></a>\n",
    "## Live Coding Tasks\n",
    "\n",
    "1. Exercise 1: [Access and Preprocess](LiveCoding/Exercise1.ipynb)\n",
    "\n",
    "2. Exercise 2: [Define and Evaluate Processing Pipelines](LiveCoding/Exercise2.ipynb) \n",
    "\n",
    "3. Exercise 3: [Hyperparameter Optimization](LiveCoding/Exercise3.ipynb) \n",
    "\n",
    "4. Exercise 4: [Implementing Neural Networks with Keras](LiveCoding/Exercise4cifar.ipynb) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
